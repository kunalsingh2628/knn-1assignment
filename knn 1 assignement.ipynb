{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843ce0c-52c9-486e-b974-68fe0fc8f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a3d10-4a2f-4995-88b0-a1bff42928b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple and versatile supervised machine learning algorithm used for both classification and regression tasks. In KNN, the prediction for a new data point is based on the majority class (for classification) or the average (for regression) of its K nearest neighbors in the training dataset. The \"nearest neighbors\" are determined based on a distance metric, typically Euclidean distance, in the feature space. KNN is instance-based and lazy learning, meaning \n",
    "it doesn't create an explicit model during the training phase but rather stores the entire training dataset for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28266e5d-ffd3-4a5f-a169-e6d242b1aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e63567-4235-467a-b818-89408b70d703",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Choosing the value of K in KNN is a crucial step, as it can significantly impact the algorithm's performance. If K is too small, the model might be sensitive to noise and outliers, leading to overfitting. If K is too large, the model might become overly biased and miss local patterns in the data. There's no one-size-fits-all rule for selecting K, but some common methods include:\n",
    "\n",
    "Odd values: Choosing an odd value of K can prevent ties when predicting the class in a binary classification problem.\n",
    "Cross-validation: Using techniques like cross-validation on a validation set to find the K value that yields the best performance.\n",
    "Grid search: Trying out a range of K values and evaluating the algorithm's performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43953ef-80d0-441a-8ffc-fd1fb116abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4553bee-ea38-41f2-bfc2-a5f030ed7b1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1975176324.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    KNN Classifier: This is used for classification tasks, where the goal is to predict the class or category that a new data point belongs to. The class is determined by the majority class among the K nearest neighbors of the data point.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "KNN Classifier: This is used for classification tasks, where the goal is to predict the class or category that a new data point belongs to. The class is determined by the majority class among the K nearest neighbors of the data point.\n",
    "\n",
    "KNN Regressor: This is used for regression tasks, where the goal is to predict a continuous numeric value for a new data point. Instead of taking the majority class, the KNN regressor calculates the average (or sometimes a weighted average) of the target values of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3913d8-ef13-4e53-8770-f0ce515cceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecb0d7-7091-4721-8a7b-82ed1ef29d59",
   "metadata": {},
   "outputs": [],
   "source": [
    " The performance of the KNN algorithm can be measured using various evaluation metrics, depending on whether you are using KNN for classification or regression:\n",
    "\n",
    "For Classification:\n",
    "\n",
    "Accuracy: The proportion of correctly classified instances among all instances.\n",
    "Precision: The ratio of true positive predictions to the total predicted positives (precision is important when false positives are costly).\n",
    "Recall: The ratio of true positive predictions to the total actual positives (recall is important when false negatives are costly).\n",
    "F1-Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "Confusion Matrix: A table showing the counts of true positive, true negative, false positive, and false negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74ddf2-88bf-4c84-94a0-adf8ab402f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "For Regression:\n",
    "\n",
    "Mean Squared Error (MSE): The average of the squared differences between the predicted and actual target values.\n",
    "Root Mean Squared Error (RMSE): The square root of the MSE, providing an error measure in the same units as the target variable.\n",
    "Mean Absolute Error (MAE): The average of the absolute differences between the predicted and actual target values.\n",
    "R-squared (Coefficient of Determination): A measure of how well the regression predictions fit the actual data, ranging from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036084e-6b40-4cf9-a306-cd8ed6cde061",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edcd3b-d100-4502-bbdd-8c9674b6bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The curse of dimensionality in KNN refers to the phenomenon where the performance of the KNN algorithm deteriorates as the number of features (dimensions) in the dataset increases. As the number of dimensions grows, the distance between data points becomes less informative,\n",
    "and the relative density of data points becomes more uniform. This can lead to issues such as increased computational complexity, higher memory usage, and decreased predictive accuracy due to the spread of data points in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218150b-dcd7-4dcd-87ce-44261fbce6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f422af-1c8c-49ef-8359-e1269f469ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imputation: Replace missing values with estimated values based on other available data points. In KNN, this could involve using the average or median of the nearest neighbors' values for imputing missing values.\n",
    "Deletion: Remove instances with missing values, but this could lead to loss of valuable data.\n",
    "Weighted KNN: Give different weights to neighbors based on their similarity and relevance, which can help in handling missing values by considering only the relevant neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de14d05-5423-480e-82a0-f1d9e5f8a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b032e0-d285-4ebb-8429-f1ae0494e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN Classifier: Better suited for problems where the target variable is categorical. It can handle complex decision boundaries and is robust to noisy data. However, it might struggle with class imbalances and requires careful choice of K to avoid overfitting or underfitting.\n",
    "KNN Regressor: More appropriate for problems with continuous numeric target variables. It captures local patterns well and is suitable when the relationship between features and target is not linear. Similar to the classifier, the choice of K is critical to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b14c05-3ec8-408f-869d-c2902de19df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f72ff2-17a9-4214-b046-22f7978dfa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Strengths:\n",
    "Simple to understand and implement.\n",
    "Non-parametric and flexible, capable of capturing complex relationships.\n",
    "Works well when decision boundaries are irregular or not well-defined.\n",
    "Can handle multi-class problems naturally.\n",
    "Weaknesses:\n",
    "Sensitive to noise and outliers.\n",
    "Computationally expensive during prediction, as it requires calculating distances for all training instances.\n",
    "Performance can degrade in high-dimensional spaces due to the curse of dimensionality.\n",
    "Requires appropriate feature scaling for accurate results.\n",
    "To address these weaknesses, strategies like outlier detection and removal, proper feature scaling, dimensionality reduction techniques (such as PCA), and efficient nearest-neighbor search algorithms (like KD-Tree or Ball-Tree) can be employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4153f75-360f-4889-bada-c28689327912",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9e077-011e-4f26-9221-48edd26d8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean Distance: This is the straight-line distance between two points in a Euclidean space. In KNN, it measures the geometric distance between two data points, considering all dimensions. It's suitable when the data features have a clear continuous relationship.\n",
    "Manhattan Distance: Also known as city block distance or L1 distance, this is the sum of absolute differences between the coordinates of two points. It's suitable when the data features are not continuous and have different scales, and when the path between points should follow axis-aligned paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d19497b-1676-4a32-938a-a747bdd81102",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea389870-b58a-4652-a7d7-b76095fb006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature scaling is essential in KNN because the algorithm's distance-based calculations are sensitive to the scale of features. Features with larger scales can dominate the distance calculation, leading to biased results. Scaling ensures that all features contribute equally to the distance metric. Common scaling methods include Min-Max scaling (normalization) and Z-score scaling (standardization), which transform features to a similar scale while preserving their relative relationships. This helps KNN work effectively and prevents features with larger scales from overpowering the prediction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a48bdc-8b9f-48b8-87ee-1f6edc14d339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52351e12-4c1e-4d6d-974d-26c7865f77e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
